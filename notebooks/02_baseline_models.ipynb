{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Maternal Health Risk Prediction - Baseline Models\n",
        "\n",
        "This notebook trains and evaluates multiple baseline machine learning models for maternal health risk classification.\n",
        "\n",
        "## Models Trained:\n",
        "1. Logistic Regression\n",
        "2. Decision Tree\n",
        "3. Random Forest\n",
        "4. Gradient Boosting\n",
        "5. XGBoost\n",
        "6. LightGBM\n",
        "\n",
        "## Evaluation Metrics:\n",
        "- Accuracy\n",
        "- Precision\n",
        "- Recall (especially for High Risk class)\n",
        "- F1 Score\n",
        "- AUC-ROC\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import sys\n",
        "sys.path.append('../src')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
        "                             f1_score, roc_auc_score, classification_report, \n",
        "                             confusion_matrix, roc_curve)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"✓ Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Processed Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load processed data\n",
        "from data_processing import load_processed_data\n",
        "\n",
        "X_train, X_val, X_test, y_train, y_val, y_test, scaler, feature_names = load_processed_data()\n",
        "\n",
        "print(f\"✓ Data loaded successfully!\")\n",
        "print(f\"  Training set: {X_train.shape}\")\n",
        "print(f\"  Validation set: {X_val.shape}\")\n",
        "print(f\"  Test set: {X_test.shape}\")\n",
        "print(f\"  Features: {feature_names}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Define Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define baseline models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced'),\n",
        "    'Decision Tree': DecisionTreeClassifier(max_depth=10, random_state=42, class_weight='balanced'),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=15, random_state=42, \n",
        "                                           class_weight='balanced', n_jobs=-1),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, \n",
        "                                                    max_depth=5, random_state=42),\n",
        "    'XGBoost': XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=5,\n",
        "                            random_state=42, eval_metric='mlogloss'),\n",
        "    'LightGBM': LGBMClassifier(n_estimators=100, learning_rate=0.1, max_depth=5,\n",
        "                              random_state=42, class_weight='balanced', verbose=-1)\n",
        "}\n",
        "\n",
        "print(f\"✓ {len(models)} models defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Train and Evaluate Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training and evaluation\n",
        "results = {}\n",
        "trained_models = {}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training {model_name}...\")\n",
        "    print('='*60)\n",
        "    \n",
        "    # Train\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    # Predictions\n",
        "    y_val_pred = model.predict(X_val)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "    y_test_proba = model.predict_proba(X_test)\n",
        "    \n",
        "    # Metrics\n",
        "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "    test_precision = precision_score(y_test, y_test_pred, average='weighted', zero_division=0)\n",
        "    test_recall = recall_score(y_test, y_test_pred, average='weighted', zero_division=0)\n",
        "    test_f1 = f1_score(y_test, y_test_pred, average='weighted', zero_division=0)\n",
        "    \n",
        "    # High Risk recall (most important)\n",
        "    high_risk_recall = recall_score(y_test, y_test_pred, labels=[2], average='macro', zero_division=0)\n",
        "    \n",
        "    # AUC\n",
        "    try:\n",
        "        test_auc = roc_auc_score(y_test, y_test_proba, multi_class='ovr', average='weighted')\n",
        "    except:\n",
        "        test_auc = 0.0\n",
        "    \n",
        "    # Store results\n",
        "    results[model_name] = {\n",
        "        'val_accuracy': val_accuracy,\n",
        "        'test_accuracy': test_accuracy,\n",
        "        'test_precision': test_precision,\n",
        "        'test_recall': test_recall,\n",
        "        'test_f1': test_f1,\n",
        "        'test_auc': test_auc,\n",
        "        'high_risk_recall': high_risk_recall\n",
        "    }\n",
        "    \n",
        "    trained_models[model_name] = model\n",
        "    \n",
        "    # Print results\n",
        "    print(f\"\\nValidation Accuracy: {val_accuracy:.4f}\")\n",
        "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "    print(f\"Test Precision: {test_precision:.4f}\")\n",
        "    print(f\"Test Recall: {test_recall:.4f}\")\n",
        "    print(f\"Test F1 Score: {test_f1:.4f}\")\n",
        "    print(f\"Test AUC: {test_auc:.4f}\")\n",
        "    print(f\"High Risk Recall: {high_risk_recall:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"✓ All models trained successfully!\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create results dataframe\n",
        "results_df = pd.DataFrame(results).T\n",
        "results_df = results_df.round(4)\n",
        "\n",
        "print(\"Model Performance Comparison:\")\n",
        "print(\"=\"*60)\n",
        "print(results_df)\n",
        "\n",
        "# Best model\n",
        "best_model_name = results_df['test_f1'].idxmax()\n",
        "best_f1_score = results_df['test_f1'].max()\n",
        "\n",
        "print(f\"\\n✓ Best Model: {best_model_name} (F1 Score: {best_f1_score:.4f})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize model comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "metrics = ['test_accuracy', 'test_precision', 'test_recall', 'test_f1']\n",
        "titles = ['Test Accuracy', 'Test Precision', 'Test Recall', 'Test F1 Score']\n",
        "\n",
        "for idx, (metric, title) in enumerate(zip(metrics, titles)):\n",
        "    ax = axes[idx // 2, idx % 2]\n",
        "    \n",
        "    values = results_df[metric].sort_values(ascending=True)\n",
        "    colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(values)))\n",
        "    \n",
        "    ax.barh(values.index, values.values, color=colors, edgecolor='black', alpha=0.8)\n",
        "    ax.set_xlabel(title, fontsize=12, fontweight='bold')\n",
        "    ax.set_title(f'{title} Comparison', fontsize=14, fontweight='bold')\n",
        "    ax.grid(True, alpha=0.3, axis='x')\n",
        "    \n",
        "    # Add value labels\n",
        "    for i, v in enumerate(values.values):\n",
        "        ax.text(v + 0.005, i, f'{v:.4f}', va='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Detailed Analysis of Best Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed evaluation of best model\n",
        "best_model = trained_models[best_model_name]\n",
        "y_test_pred_best = best_model.predict(X_test)\n",
        "\n",
        "print(f\"Detailed Analysis of {best_model_name}\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_test_pred_best, target_names=['Low Risk', 'Mid Risk', 'High Risk']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_test_pred_best)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['Low Risk', 'Mid Risk', 'High Risk'],\n",
        "            yticklabels=['Low Risk', 'Mid Risk', 'High Risk'],\n",
        "            cbar_kws={'label': 'Count'})\n",
        "plt.title(f'Confusion Matrix - {best_model_name}', fontsize=14, fontweight='bold', pad=20)\n",
        "plt.ylabel('True Label', fontsize=12)\n",
        "plt.xlabel('Predicted Label', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature importance (if available)\n",
        "if hasattr(best_model, 'feature_importances_'):\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Importance': best_model.feature_importances_\n",
        "    }).sort_values('Importance', ascending=False)\n",
        "    \n",
        "    print(\"\\nFeature Importance:\")\n",
        "    print(feature_importance)\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(data=feature_importance, x='Importance', y='Feature', palette='viridis')\n",
        "    plt.title(f'Feature Importance - {best_model_name}', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Importance Score', fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"\\nFeature importance not available for this model type\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Confusion Matrices for All Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion matrices for all models\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx, (model_name, model) in enumerate(trained_models.items()):\n",
        "    y_pred = model.predict(X_test)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    \n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
        "               xticklabels=['Low', 'Mid', 'High'],\n",
        "               yticklabels=['Low', 'Mid', 'High'],\n",
        "               cbar_kws={'label': 'Count'})\n",
        "    axes[idx].set_title(f'{model_name}', fontsize=12, fontweight='bold')\n",
        "    axes[idx].set_ylabel('True Label')\n",
        "    axes[idx].set_xlabel('Predicted Label')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "Key findings from baseline model training:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"BASELINE MODELS SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\n✓ Best Model: {best_model_name}\")\n",
        "print(f\"  F1 Score: {results[best_model_name]['test_f1']:.4f}\")\n",
        "print(f\"  Accuracy: {results[best_model_name]['test_accuracy']:.4f}\")\n",
        "print(f\"  High Risk Recall: {results[best_model_name]['high_risk_recall']:.4f}\")\n",
        "\n",
        "print(\"\\n✓ Top 3 Models by F1 Score:\")\n",
        "top3 = results_df.nlargest(3, 'test_f1')[['test_f1', 'test_accuracy', 'high_risk_recall']]\n",
        "print(top3)\n",
        "\n",
        "print(\"\\n✓ Next Steps:\")\n",
        "print(\"  1. Train deep learning model for comparison\")\n",
        "print(\"  2. Apply SHAP and LIME for explainability\")\n",
        "print(\"  3. Select final model for deployment\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
