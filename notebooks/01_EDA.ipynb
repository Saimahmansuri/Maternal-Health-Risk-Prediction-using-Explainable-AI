{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Maternal Health Risk Prediction - Exploratory Data Analysis\n",
        "\n",
        "This notebook performs comprehensive exploratory data analysis on the maternal health risk dataset.\n",
        "\n",
        "## Contents\n",
        "1. Data Loading\n",
        "2. Data Quality Assessment\n",
        "3. Descriptive Statistics\n",
        "4. Feature Distributions\n",
        "5. Correlation Analysis\n",
        "6. Class Balance Analysis\n",
        "7. Feature Relationships\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading\n",
        "\n",
        "Load the raw maternal health risk dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "data_path = '../data/raw/maternal_health.csv'\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(data_path)\n",
        "    print(f\"✓ Data loaded successfully!\")\n",
        "    print(f\"  Shape: {df.shape}\")\n",
        "    print(f\"  Rows: {df.shape[0]}\")\n",
        "    print(f\"  Columns: {df.shape[1]}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"⚠ Dataset not found!\")\n",
        "    print(\"Please download the dataset from:\")\n",
        "    print(\"  - Kaggle: https://www.kaggle.com/datasets/andrewmvd/maternal-health-risk-data\")\n",
        "    print(\"  - UCI: https://archive.ics.uci.edu/ml/datasets/Maternal+Health+Risk+Data+Set\")\n",
        "    print(f\"And place it at: {data_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display first few rows\n",
        "df.head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Quality Assessment\n",
        "\n",
        "Check for missing values, duplicates, and data types.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data info\n",
        "print(\"Dataset Information:\")\n",
        "print(\"=\"*60)\n",
        "df.info()\n",
        "print(\"\\n\" + \"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print(\"Missing Values:\")\n",
        "print(\"=\"*60)\n",
        "missing = df.isnull().sum()\n",
        "missing_pct = (df.isnull().sum() / len(df)) * 100\n",
        "\n",
        "missing_df = pd.DataFrame({\n",
        "    'Missing Count': missing,\n",
        "    'Percentage': missing_pct\n",
        "})\n",
        "\n",
        "print(missing_df)\n",
        "\n",
        "if missing.sum() == 0:\n",
        "    print(\"\\n✓ No missing values found!\")\n",
        "else:\n",
        "    print(f\"\\n⚠ Total missing values: {missing.sum()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for duplicates\n",
        "duplicates = df.duplicated().sum()\n",
        "print(f\"Duplicate rows: {duplicates}\")\n",
        "\n",
        "if duplicates > 0:\n",
        "    print(f\"⚠ {duplicates} duplicate rows found ({duplicates/len(df)*100:.2f}%)\")\n",
        "else:\n",
        "    print(\"✓ No duplicate rows found!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Descriptive Statistics\n",
        "\n",
        "Compute summary statistics for all features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Descriptive statistics\n",
        "print(\"Descriptive Statistics:\")\n",
        "print(\"=\"*60)\n",
        "df.describe().round(2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Target variable distribution\n",
        "print(\"Target Variable Distribution (RiskLevel):\")\n",
        "print(\"=\"*60)\n",
        "risk_counts = df['RiskLevel'].value_counts()\n",
        "risk_pct = (df['RiskLevel'].value_counts(normalize=True) * 100).round(2)\n",
        "\n",
        "target_df = pd.DataFrame({\n",
        "    'Count': risk_counts,\n",
        "    'Percentage': risk_pct\n",
        "})\n",
        "\n",
        "print(target_df)\n",
        "print(f\"\\nTotal samples: {len(df)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Feature Distributions\n",
        "\n",
        "Visualize the distribution of each feature.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create histograms for all numerical features\n",
        "numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx, col in enumerate(numerical_cols):\n",
        "    axes[idx].hist(df[col], bins=30, edgecolor='black', alpha=0.7, color='skyblue')\n",
        "    axes[idx].set_title(f'Distribution of {col}', fontsize=12, fontweight='bold')\n",
        "    axes[idx].set_xlabel(col)\n",
        "    axes[idx].set_ylabel('Frequency')\n",
        "    axes[idx].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Add mean line\n",
        "    mean_val = df[col].mean()\n",
        "    axes[idx].axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_val:.2f}')\n",
        "    axes[idx].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Box plots to identify outliers\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx, col in enumerate(numerical_cols):\n",
        "    axes[idx].boxplot(df[col], vert=True, patch_artist=True,\n",
        "                     boxprops=dict(facecolor='lightblue', alpha=0.7))\n",
        "    axes[idx].set_title(f'Box Plot of {col}', fontsize=12, fontweight='bold')\n",
        "    axes[idx].set_ylabel(col)\n",
        "    axes[idx].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Correlation Analysis\n",
        "\n",
        "Examine relationships between features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Select only numerical columns\n",
        "numerical_df = df.select_dtypes(include=[np.number])\n",
        "correlation = numerical_df.corr()\n",
        "\n",
        "# Create heatmap\n",
        "sns.heatmap(correlation, annot=True, fmt='.2f', cmap='coolwarm', \n",
        "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
        "\n",
        "plt.title('Feature Correlation Heatmap', fontsize=16, fontweight='bold', pad=20)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nHighest Correlations (excluding diagonal):\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Get correlation pairs\n",
        "corr_pairs = []\n",
        "for i in range(len(correlation.columns)):\n",
        "    for j in range(i+1, len(correlation.columns)):\n",
        "        corr_pairs.append((correlation.columns[i], correlation.columns[j], correlation.iloc[i, j]))\n",
        "\n",
        "# Sort by absolute correlation\n",
        "corr_pairs_sorted = sorted(corr_pairs, key=lambda x: abs(x[2]), reverse=True)\n",
        "\n",
        "for feat1, feat2, corr_val in corr_pairs_sorted[:5]:\n",
        "    print(f\"  {feat1} <-> {feat2}: {corr_val:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Class Balance Analysis\n",
        "\n",
        "Analyze the distribution of risk levels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Risk level distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Bar plot\n",
        "risk_counts = df['RiskLevel'].value_counts()\n",
        "axes[0].bar(risk_counts.index, risk_counts.values, color=['green', 'orange', 'red'], alpha=0.7, edgecolor='black')\n",
        "axes[0].set_title('Risk Level Distribution (Count)', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Risk Level')\n",
        "axes[0].set_ylabel('Count')\n",
        "axes[0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add value labels on bars\n",
        "for i, v in enumerate(risk_counts.values):\n",
        "    axes[0].text(i, v + 5, str(v), ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# Pie chart\n",
        "colors = ['green', 'orange', 'red']\n",
        "axes[1].pie(risk_counts.values, labels=risk_counts.index, autopct='%1.1f%%', \n",
        "           colors=colors, startangle=90, explode=[0.05, 0.05, 0.05])\n",
        "axes[1].set_title('Risk Level Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Check for class imbalance\n",
        "print(\"\\nClass Balance Analysis:\")\n",
        "print(\"=\"*60)\n",
        "total = len(df)\n",
        "for risk_level in risk_counts.index:\n",
        "    count = risk_counts[risk_level]\n",
        "    pct = (count / total) * 100\n",
        "    print(f\"{risk_level}: {count} ({pct:.2f}%)\")\n",
        "\n",
        "# Calculate imbalance ratio\n",
        "max_class = risk_counts.max()\n",
        "min_class = risk_counts.min()\n",
        "imbalance_ratio = max_class / min_class\n",
        "\n",
        "print(f\"\\nImbalance Ratio: {imbalance_ratio:.2f}:1\")\n",
        "if imbalance_ratio > 2:\n",
        "    print(\"⚠ Significant class imbalance detected! Consider using:\")\n",
        "    print(\"  - Class weights\")\n",
        "    print(\"  - SMOTE or other resampling techniques\")\n",
        "    print(\"  - Stratified sampling\")\n",
        "else:\n",
        "    print(\"✓ Classes are relatively balanced\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Feature Relationships with Target\n",
        "\n",
        "Analyze how each feature relates to the risk level.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Box plots of features by risk level\n",
        "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx, col in enumerate(numerical_cols):\n",
        "    df.boxplot(column=col, by='RiskLevel', ax=axes[idx], \n",
        "               patch_artist=True, grid=True)\n",
        "    axes[idx].set_title(f'{col} by Risk Level', fontsize=12, fontweight='bold')\n",
        "    axes[idx].set_xlabel('Risk Level')\n",
        "    axes[idx].set_ylabel(col)\n",
        "    axes[idx].get_figure().suptitle('')  # Remove automatic title\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Violin plots for better distribution visualization\n",
        "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "risk_order = ['low risk', 'mid risk', 'high risk']\n",
        "\n",
        "for idx, col in enumerate(numerical_cols):\n",
        "    sns.violinplot(data=df, x='RiskLevel', y=col, order=risk_order, \n",
        "                  palette=['green', 'orange', 'red'], ax=axes[idx])\n",
        "    axes[idx].set_title(f'{col} Distribution by Risk Level', fontsize=12, fontweight='bold')\n",
        "    axes[idx].set_xlabel('Risk Level')\n",
        "    axes[idx].set_ylabel(col)\n",
        "    axes[idx].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mean values by risk level\n",
        "print(\"Mean Feature Values by Risk Level:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "mean_by_risk = df.groupby('RiskLevel')[numerical_cols].mean().round(2)\n",
        "print(mean_by_risk)\n",
        "\n",
        "# Calculate differences between risk levels\n",
        "print(\"\\n\\nFeature Differences (High Risk - Low Risk):\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if 'high risk' in mean_by_risk.index and 'low risk' in mean_by_risk.index:\n",
        "    diff = (mean_by_risk.loc['high risk'] - mean_by_risk.loc['low risk']).sort_values(ascending=False)\n",
        "    print(diff)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Pair Plot\n",
        "\n",
        "Comprehensive view of feature relationships colored by risk level.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pair plot (may take a moment to render)\n",
        "print(\"Generating pair plot... (this may take a moment)\")\n",
        "\n",
        "# Select a subset of features for clarity\n",
        "features_subset = ['Age', 'SystolicBP', 'DiastolicBP', 'BS', 'RiskLevel']\n",
        "\n",
        "if all(col in df.columns for col in features_subset[:-1]):\n",
        "    pairplot = sns.pairplot(df[features_subset], hue='RiskLevel', \n",
        "                           palette=['green', 'orange', 'red'],\n",
        "                           diag_kind='kde', plot_kws={'alpha': 0.6})\n",
        "    pairplot.fig.suptitle('Feature Pair Plot by Risk Level', y=1.02, fontsize=16, fontweight='bold')\n",
        "    plt.show()\n",
        "    print(\"✓ Pair plot generated successfully!\")\n",
        "else:\n",
        "    print(\"⚠ Some features not found in dataset\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Findings Summary\n",
        "\n",
        "Based on the exploratory data analysis:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"KEY FINDINGS FROM EDA\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\n1. DATA QUALITY:\")\n",
        "print(f\"   - Dataset shape: {df.shape}\")\n",
        "print(f\"   - Missing values: {df.isnull().sum().sum()}\")\n",
        "print(f\"   - Duplicate rows: {df.duplicated().sum()}\")\n",
        "\n",
        "print(\"\\n2. TARGET DISTRIBUTION:\")\n",
        "risk_dist = df['RiskLevel'].value_counts()\n",
        "for risk, count in risk_dist.items():\n",
        "    print(f\"   - {risk}: {count} ({count/len(df)*100:.1f}%)\")\n",
        "\n",
        "print(\"\\n3. FEATURE CHARACTERISTICS:\")\n",
        "for col in numerical_cols:\n",
        "    print(f\"   - {col}:\")\n",
        "    print(f\"     Mean: {df[col].mean():.2f}, Std: {df[col].std():.2f}\")\n",
        "    print(f\"     Range: [{df[col].min():.2f}, {df[col].max():.2f}]\")\n",
        "\n",
        "print(\"\\n4. RECOMMENDATIONS:\")\n",
        "print(\"   ✓ Dataset is clean and ready for modeling\")\n",
        "print(\"   ✓ Consider feature scaling due to different ranges\")\n",
        "if df.duplicated().sum() > 0:\n",
        "    print(\"   ⚠ Remove duplicate rows before modeling\")\n",
        "if imbalance_ratio > 2:\n",
        "    print(\"   ⚠ Use class weights or resampling for imbalanced classes\")\n",
        "print(\"   ✓ Stratified splitting recommended for train/val/test\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EDA COMPLETE!\")\n",
        "print(\"=\"*60)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
